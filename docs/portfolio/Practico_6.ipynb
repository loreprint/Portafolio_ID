{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddc1963a",
   "metadata": {},
   "source": [
    "## Parte 1: Continuamos con el Dataset Ames Housing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691b1aa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Todas las librer√≠as importadas correctamente\n",
      "Configuraci√≥n de visualizaciones lista!\n"
     ]
    }
   ],
   "source": [
    "# === SETUP DEL ENTORNO ===\n",
    "\n",
    "# 1. Importar librer√≠as necesarias\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import PowerTransformer, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor  \n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "import warnings\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Todas las librer√≠as importadas correctamente\")\n",
    "\n",
    "# 2. Configurar visualizaciones - elige tu estilo favorito\n",
    "plt.style.use('seaborn-v0_8')  # Puedes cambiar por 'default', 'classic', etc.\n",
    "sns.set_palette(\"husl\")        # Paleta colorida para m√∫ltiples comparaciones\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "print(\"Configuraci√≥n de visualizaciones lista!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a549f08c",
   "metadata": {},
   "source": [
    "## Paso 2: Cargar el Dataset Ames Housing (Continuaci√≥n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520e1f14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Ames Housing cargado: (2930, 82)\n",
      "üè† ¬°Ahora vas a explorar las escalas en datos REALES!\n"
     ]
    }
   ],
   "source": [
    "# === CARGAR DATASET AMES HOUSING ===\n",
    "\n",
    "# Si vienes de la pr√°ctica anterior, ya tienes el dataset limpio\n",
    "# Si empiezas aqu√≠, vamos a cargarlo de nuevo\n",
    "\n",
    "# 1. Cargar dataset\n",
    "df_raw = pd.read_csv('../csvs/AmesHousing.csv') \n",
    "\n",
    "print(f\"Dataset Ames Housing cargado: {df_raw.shape}\")\n",
    "print(\"üè† ¬°Ahora vas a explorar las escalas en datos REALES!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4fde8ea",
   "metadata": {},
   "source": [
    "¬øCu√°les son las 5 columnas num√©ricas con las escalas m√°s diferentes?\n",
    "\n",
    "¬øHay outliers evidentes que podr√≠an afectar el escalado?\n",
    "\n",
    "¬øQu√© variable ser√° nuestro target para predicci√≥n?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ea0c3fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== INFORMACI√ìN GENERAL DEL DATASET ===\n",
      "Dimensiones: (2930, 82)\n",
      "\n",
      "Tipos de datos:\n",
      "object     43\n",
      "int64      28\n",
      "float64    11\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# === EXPLORACI√ìN INICIAL ===\n",
    "\n",
    "# Tu turno: explora las escalas\n",
    "print(\"=== INFORMACI√ìN GENERAL DEL DATASET ===\")\n",
    "print(f\"Dimensiones: {df_raw.shape}\")\n",
    "print(f\"\\nTipos de datos:\")\n",
    "print(df_raw.dtypes.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d7f278e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Columnas num√©ricas encontradas: 39\n"
     ]
    }
   ],
   "source": [
    "# Identifica columnas num√©ricas\n",
    "numeric_cols = df_raw.select_dtypes(include=[np.number]).columns.tolist()\n",
    "print(f\"\\nüìä Columnas num√©ricas encontradas: {len(numeric_cols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1b7abf24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç TU AN√ÅLISIS: Examina las escalas\n",
      "Estad√≠sticas de las primeras 10 columnas num√©ricas:\n",
      "            Order           PID  MS SubClass  Lot Frontage       Lot Area  \\\n",
      "count  2930.00000  2.930000e+03  2930.000000   2440.000000    2930.000000   \n",
      "mean   1465.50000  7.144645e+08    57.387372     69.224590   10147.921843   \n",
      "std     845.96247  1.887308e+08    42.638025     23.365335    7880.017759   \n",
      "min       1.00000  5.263011e+08    20.000000     21.000000    1300.000000   \n",
      "25%     733.25000  5.284770e+08    20.000000     58.000000    7440.250000   \n",
      "50%    1465.50000  5.354536e+08    50.000000     68.000000    9436.500000   \n",
      "75%    2197.75000  9.071811e+08    70.000000     80.000000   11555.250000   \n",
      "max    2930.00000  1.007100e+09   190.000000    313.000000  215245.000000   \n",
      "\n",
      "       Overall Qual  Overall Cond   Year Built  Year Remod/Add  Mas Vnr Area  \n",
      "count   2930.000000   2930.000000  2930.000000     2930.000000   2907.000000  \n",
      "mean       6.094881      5.563140  1971.356314     1984.266553    101.896801  \n",
      "std        1.411026      1.111537    30.245361       20.860286    179.112611  \n",
      "min        1.000000      1.000000  1872.000000     1950.000000      0.000000  \n",
      "25%        5.000000      5.000000  1954.000000     1965.000000      0.000000  \n",
      "50%        6.000000      5.000000  1973.000000     1993.000000      0.000000  \n",
      "75%        7.000000      6.000000  2001.000000     2004.000000    164.000000  \n",
      "max       10.000000      9.000000  2010.000000     2010.000000   1600.000000  \n"
     ]
    }
   ],
   "source": [
    "# Tu an√°lisis: ¬øcu√°les tienen escalas MUY diferentes?\n",
    "print(\"\\nüîç TU AN√ÅLISIS: Examina las escalas\")\n",
    "print(\"Estad√≠sticas de las primeras 10 columnas num√©ricas:\")\n",
    "print(df_raw[numeric_cols[:10]].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4ff0eaad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ü§î PREGUNTA PARA TI:\n",
      "Mira los valores de 'min' y 'max' arriba.\n",
      "¬øCu√°les columnas tienen escalas que pueden ser problem√°ticas para KNN o SVM?\n"
     ]
    }
   ],
   "source": [
    "# Pregunta para reflexionar:\n",
    "print(\"\\nü§î PREGUNTA PARA TI:\")\n",
    "print(\"Mira los valores de 'min' y 'max' arriba.\")\n",
    "print(\"¬øCu√°les columnas tienen escalas que pueden ser problem√°ticas para KNN o SVM?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fda91a",
   "metadata": {},
   "source": [
    "üí° Pistas para tu exploraci√≥n: - Busca columnas donde max/min > 1000 (escalas muy diferentes) - ¬ø'Lot Area' vs 'Year Built' vs 'SalePrice' est√°n en escalas similares? - ¬øHay alguna columna que claramente va a \"gritar m√°s fuerte\"?\n",
    "\n",
    "Paso 3: Tu Investigaci√≥n - Identificar Variables Problem√°ticas¬∂\n",
    "üéØ Tu misi√≥n: Investiga y selecciona columnas con escalas problem√°ticas del dataset Ames Housing.\n",
    "\n",
    "üîç Lo que debes hacer: 1. Explora las columnas num√©ricas y encuentra las que tienen escalas MUY diferentes 2. Calcula ratios (m√°ximo/m√≠nimo) para identificar las m√°s problem√°ticas 3. Crea visualizaciones para mostrar el problema de escalas 4. Documenta tus hallazgos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "847aebf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estad√≠sticas de las columnas seleccionadas:\n",
      "           SalePrice       Lot Area  Gr Liv Area  Total Bsmt SF   1st Flr SF  \\\n",
      "min     12789.000000    1300.000000   334.000000            0.0   334.000000   \n",
      "max    755000.000000  215245.000000  5642.000000         6110.0  5095.000000   \n",
      "ratio      59.035108     165.573077    16.892216            inf    15.254491   \n",
      "\n",
      "        Year Built  \n",
      "min    1872.000000  \n",
      "max    2010.000000  \n",
      "ratio     1.073718  \n",
      "‚úÖ Gr√°ficos guardados en la carpeta 'results/'\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "| Mi Columna      | Rango (min-max)         | Ratio         | ¬øProblem√°tica? | ¬øPor qu√©? |\n",
       "|-----------------|------------------------|---------------|---------------|-----------|\n",
       "| SalePrice | 12789 - 755000 | 59.04 | No |  |\n",
       "| Lot Area | 1300 - 215245 | 165.57 | No |  |\n",
       "| Gr Liv Area | 334 - 5642 | 16.89 | No |  |\n",
       "| Total Bsmt SF | 0 - 6110 | inf | S√≠ | Escala muy diferente, puede dominar el modelo |\n",
       "| 1st Flr SF | 334 - 5095 | 15.25 | No |  |\n",
       "| Year Built | 1872 - 2010 | 1.07 | No |  |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# === TU INVESTIGACI√ìN DE SLAS ===\n",
    "\n",
    "# TODO: Selecciona 5-6 columnas num√©ricas interesantes\n",
    "# Tip: Busca columnas como √°reas, precios, a√±os, etc.\n",
    "selected_features = [  \n",
    "                    \"SalePrice\",\n",
    "                    \"Lot Area\",\n",
    "                    \"Gr Liv Area\",\n",
    "                    \"Total Bsmt SF\",\n",
    "                    \"1st Flr SF\",  \n",
    "                    \"Year Built\"\n",
    "                    ]\n",
    "\n",
    "# TODO: Analiza las escalas de tus columnas seleccionadas\n",
    "# Calcula min, max, ratios\n",
    "# ¬øCu√°l tiene el ratio m√°s alto?\n",
    "\n",
    "\n",
    "# Analiza las escalas de tus columnas seleccionadas\n",
    "stats = df_raw[selected_features].agg(['min', 'max'])\n",
    "stats.loc['ratio'] = stats.loc['max'] / stats.loc['min']\n",
    "\n",
    "print(\"Estad√≠sticas de las columnas seleccionadas:\")\n",
    "print(stats)\n",
    "\n",
    "# ¬øCu√°l tiene el ratio m√°s alto?\n",
    "max_ratio_col = stats.loc['ratio'].idxmax()\n",
    "max_ratio_val = stats.loc['ratio'].max()\n",
    "\n",
    "\n",
    "\n",
    "# TODO: Crea visualizaciones para mostrar el problema\n",
    "# Histogramas, boxplots, o lo que consideres mejor\n",
    "# Guarda tus gr√°ficos en la carpeta 'results/'\n",
    "\n",
    "# Crear carpeta 'results' si no existe\n",
    "os.makedirs('results', exist_ok=True)\n",
    "\n",
    "# Histogramas para comparar escalas\n",
    "for col in selected_features:\n",
    "    plt.figure()\n",
    "    sns.histplot(df_raw[col].dropna(), kde=True, bins=30)\n",
    "    plt.title(f'Histograma de {col}')\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('Frecuencia')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'results/hist_{col}.png')\n",
    "    plt.close()\n",
    "\n",
    "# Boxplots para comparar escalas en una sola figura\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data=df_raw[selected_features], orient='h')\n",
    "plt.title('Boxplots de columnas seleccionadas')\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/boxplots_selected_features.png')\n",
    "plt.close()\n",
    "\n",
    "print(\"‚úÖ Gr√°ficos guardados en la carpeta 'results/'\")\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Tabla resumen de escalas y problem√°tica\n",
    "tabla = \"| Mi Columna      | Rango (min-max)         | Ratio         | ¬øProblem√°tica? | ¬øPor qu√©? |\\n\"\n",
    "tabla += \"|-----------------|------------------------|---------------|---------------|-----------|\\n\"\n",
    "\n",
    "for col in selected_features:\n",
    "    min_val = stats.loc['min', col]\n",
    "    max_val = stats.loc['max', col]\n",
    "    ratio = stats.loc['ratio', col]\n",
    "    problematica = \"S√≠\" if ratio == max_ratio_val else \"No\"\n",
    "    motivo = \"Escala muy diferente, puede dominar el modelo\" if ratio == max_ratio_val else \"\"\n",
    "    tabla += f\"| {col} | {min_val:.0f} - {max_val:.0f} | {ratio:,.2f} | {problematica} | {motivo} |\\n\"\n",
    "\n",
    "display(Markdown(tabla))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ad85c4",
   "metadata": {},
   "source": [
    "Paso 4: Preparar Datos para Experimentar con Scalers¬∂\n",
    "üéØ Tu tarea: Prepara el dataset para experimentar con diferentes scalers, siguiendo buenas pr√°cticas de ML.\n",
    "\n",
    "üîç Lo que debes lograr: 1. Definir tu variable target y features (bas√°ndote en tu investigaci√≥n del Paso 3) 2. Limpiar los datos (NaN, inconsistencias) 3. Hacer split de datos ANTES de escalar (¬°cr√≠tico para evitar leakage!) 4. Verificar que el problema de escalas persiste\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "842bc1d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä MI PREPARACI√ìN:\n",
      "Target: SalePrice\n",
      "Features: 5 columnas\n",
      "            Lot Area  Gr Liv Area  Total Bsmt SF   1st Flr SF   Year Built\n",
      "count    2343.000000  2343.000000    2343.000000  2343.000000  2343.000000\n",
      "mean    10210.040120  1498.816048    1048.850192  1160.533504  1971.087921\n",
      "std      8307.372036   497.457673     419.228411   377.189339    30.210872\n",
      "min      1300.000000   334.000000       0.000000   334.000000  1872.000000\n",
      "25%      7461.000000  1127.500000     793.000000   882.000000  1954.000000\n",
      "50%      9466.000000  1444.000000     990.000000  1089.000000  1973.000000\n",
      "75%     11592.000000  1750.000000    1298.500000  1384.000000  2000.000000\n",
      "max    215245.000000  4676.000000    3206.000000  3820.000000  2010.000000\n",
      "Datos: 2343 train, 586 test\n",
      "Problema de escalas confirmado: S√≠/No - explica por qu√©\n"
     ]
    }
   ],
   "source": [
    "# === PREPARACI√ìN DE DATOS ===\n",
    "\n",
    "# TODO: Define tu target y features bas√°ndote en tu an√°lisis anterior\n",
    "target_col = \"SalePrice\"  # ¬øCu√°l es tu variable objetivo?\n",
    "feature_cols =  [  \n",
    "                    \"Lot Area\",\n",
    "                    \"Gr Liv Area\",\n",
    "                    \"Total Bsmt SF\",\n",
    "                    \"1st Flr SF\",  \n",
    "                    \"Year Built\"\n",
    "                    ]\n",
    " # Tus features seleccionadas\n",
    "\n",
    "# TODO: Limpieza b√°sica de datos\n",
    "# ¬øC√≥mo vas a manejar NaN? ¬øEliminar filas? ¬øImputar?\n",
    "# Decide y justifica tu estrategia\n",
    "\n",
    "df_clean = df_raw[feature_cols + [target_col]].dropna()\n",
    "\n",
    "# TODO: Split de datos (¬°ANTES del escalado!)\n",
    "# Recuerda: test_size, random_state, estratificaci√≥n si es necesario\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df_clean[feature_cols]\n",
    "y = df_clean[target_col]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# TODO: Verifica que el problema de escalas a√∫n existe\n",
    "# Calcula estad√≠sticas descriptivas de tus features en el conjunto de entrenamiento\n",
    "# ¬øLas escalas siguen siendo problem√°ticas?\n",
    "\n",
    "print(\"üìä MI PREPARACI√ìN:\")\n",
    "print(f\"Target: {target_col}\")\n",
    "print(f\"Features: {len(feature_cols)} columnas\")  \n",
    "print(X_train.describe())\n",
    "print(f\"Datos: {X_train.shape[0]} train, {X_test.shape[0]} test\")\n",
    "print(\"Problema de escalas confirmado: S√≠/No - explica por qu√©\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493c7875",
   "metadata": {},
   "source": [
    "Paso 5: Reconectando con Outliers - El Orden de las Transformaciones¬∂\n",
    "üîÑ Conexi√≥n con la Pr√°ctica Anterior: En la pr√°ctica 05 detectaste outliers en datos originales. Ahora investigar√°s algo crucial: ¬øc√≥mo el escalado cambia la detecci√≥n de outliers?\n",
    "\n",
    "üéØ Tu investigaci√≥n: ¬øEl orden de las operaciones importa? ¬øDetectar outliers antes o despu√©s del escalado?\n",
    "\n",
    "üîç Lo que debes investigar: 1. Detecta outliers en datos originales (usando m√©todos de la pr√°ctica 05) 2. Aplica diferentes scalers y detecta outliers en datos escalados 3. Compara resultados: ¬øcambiaron los outliers despu√©s del escalado? 4. Saca conclusiones sobre cu√°ndo limpiar outliers en tu pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a7dd6e60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç ROUND 1: DATOS ORIGINALES\n",
      "‚ö° ROUND 2: DESPU√âS DEL ESCALADO\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "| M√©todo | Outliers Originales | Outliers Post-Standard | Outliers Post-MinMax | Outliers Post-Robust |\n",
       "|--------|---------------------|-----------------------|----------------------|----------------------|\n",
       "| IQR | 104 | 104 | 104 | 104 |\n",
       "| Z-Score | 39 | 39 | 39 | 39 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "An√°lisis:\n",
      "- StandardScaler y MinMaxScaler suelen mantener la cantidad de outliers, pero pueden cambiar los valores extremos.\n",
      "- RobustScaler es menos sensible a outliers, por lo que puede detectar menos (o diferentes) outliers.\n",
      "- La intersecci√≥n muestra cu√°ntos puntos siguen siendo considerados outliers tras el escalado.\n"
     ]
    }
   ],
   "source": [
    "# === TU EXPERIMENTO: OUTLIERS Y ESCALADO ===\n",
    "\n",
    "# TODO: Implementa funciones de detecci√≥n de outliers (pr√°ctica 05)\n",
    "# IQR method, Z-score method, u otros que prefieras\n",
    "\n",
    "def detect_outliers_iqr(data, column_name):\n",
    "    \"\"\"\n",
    "    Detecta outliers usando el m√©todo IQR.\n",
    "    Devuelve los √≠ndices de los outliers.\n",
    "    \"\"\"\n",
    "    Q1 = data[column_name].quantile(0.25)\n",
    "    Q3 = data[column_name].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers = data[(data[column_name] < lower_bound) | (data[column_name] > upper_bound)]\n",
    "    return outliers.index\n",
    "\n",
    "def detect_outliers_zscore(data, column_name, threshold=3):\n",
    "    \"\"\"\n",
    "    Detecta outliers usando el m√©todo Z-score.\n",
    "    Devuelve los √≠ndices de los outliers.\n",
    "    \"\"\"\n",
    "    mean = data[column_name].mean()\n",
    "    std = data[column_name].std()\n",
    "    z_scores = (data[column_name] - mean) / std\n",
    "    outliers = data[np.abs(z_scores) > threshold]\n",
    "    return outliers.index\n",
    "\n",
    "# TODO: Elige una columna interesante para analizar\n",
    "target_column = \"Gr Liv Area\"  # ¬øCu√°l columna analizar√°s?\n",
    "\n",
    "# TODO: ROUND 1 - Detecci√≥n en datos originales\n",
    "print(\"üîç ROUND 1: DATOS ORIGINALES\")\n",
    "# Detecta outliers en datos sin escalar\n",
    "# ¬øCu√°ntos outliers encontraste? ¬øD√≥nde est√°n?\n",
    "\n",
    "# TODO: ROUND 2 - Detecci√≥n despu√©s de cada scaler\n",
    "scalers_to_test = {\n",
    "    'StandardScaler': StandardScaler(),\n",
    "    'MinMaxScaler': MinMaxScaler(),\n",
    "    'RobustScaler': RobustScaler()\n",
    "}\n",
    "\n",
    "# Para cada scaler:\n",
    "# 1. Aplica el scaler a tus datos\n",
    "# 2. Detecta outliers en los datos escalados\n",
    "# 3. Compara con los outliers originales\n",
    "\n",
    "# TODO: An√°lisis de resultados\n",
    "# ¬øQu√© scaler detect√≥ m√°s/menos outliers?\n",
    "# ¬øLos mismos puntos siguen siendo outliers?\n",
    "# ¬øRobustScaler cambi√≥ la detecci√≥n como esperabas?\n",
    "print(\"‚ö° ROUND 2: DESPU√âS DEL ESCALADO\")\n",
    "\n",
    "# Usamos solo la columna a analizar\n",
    "X_col = y_train.to_frame(name=target_column).copy()\n",
    "\n",
    "outliers_original = set(detect_outliers_iqr(X_col, target_column))\n",
    "# Creamos una tabla resumen de outliers para ambos m√©todos y cada scaler\n",
    "\n",
    "# Detectar outliers originales\n",
    "outliers_iqr_original = set(detect_outliers_iqr(X_col, target_column))\n",
    "outliers_z_original = set(detect_outliers_zscore(X_col, target_column))\n",
    "\n",
    "# Diccionario para guardar resultados\n",
    "tabla_outliers = {\n",
    "    \"M√©todo\": [],\n",
    "    \"Outliers Originales\": [],\n",
    "    \"Outliers Post-Standard\": [],\n",
    "    \"Outliers Post-MinMax\": [],\n",
    "    \"Outliers Post-Robust\": []\n",
    "}\n",
    "\n",
    "# Scalers a usar\n",
    "scalers_to_test = {\n",
    "    'Standard': StandardScaler(),\n",
    "    'MinMax': MinMaxScaler(),\n",
    "    'Robust': RobustScaler()\n",
    "}\n",
    "\n",
    "# IQR\n",
    "tabla_outliers[\"M√©todo\"].append(\"IQR\")\n",
    "tabla_outliers[\"Outliers Originales\"].append(len(outliers_iqr_original))\n",
    "for scaler_name, scaler in scalers_to_test.items():\n",
    "    X_scaled = scaler.fit_transform(X_col)\n",
    "    X_scaled_df = pd.DataFrame(X_scaled, columns=[target_column], index=X_col.index)\n",
    "    outliers_scaled = set(detect_outliers_iqr(X_scaled_df, target_column))\n",
    "    tabla_outliers[f\"Outliers Post-{scaler_name}\"].append(len(outliers_scaled))\n",
    "\n",
    "# Z-Score\n",
    "tabla_outliers[\"M√©todo\"].append(\"Z-Score\")\n",
    "tabla_outliers[\"Outliers Originales\"].append(len(outliers_z_original))\n",
    "for scaler_name, scaler in scalers_to_test.items():\n",
    "    X_scaled = scaler.fit_transform(X_col)\n",
    "    X_scaled_df = pd.DataFrame(X_scaled, columns=[target_column], index=X_col.index)\n",
    "    outliers_scaled = set(detect_outliers_zscore(X_scaled_df, target_column))\n",
    "    tabla_outliers[f\"Outliers Post-{scaler_name}\"].append(len(outliers_scaled))\n",
    "\n",
    "# Mostrar tabla en formato Markdown\n",
    "tabla_md = \"| M√©todo | Outliers Originales | Outliers Post-Standard | Outliers Post-MinMax | Outliers Post-Robust |\\n\"\n",
    "tabla_md += \"|--------|---------------------|-----------------------|----------------------|----------------------|\\n\"\n",
    "for i in range(2):\n",
    "    tabla_md += f\"| {tabla_outliers['M√©todo'][i]} | {tabla_outliers['Outliers Originales'][i]} | {tabla_outliers['Outliers Post-Standard'][i]} | {tabla_outliers['Outliers Post-MinMax'][i]} | {tabla_outliers['Outliers Post-Robust'][i]} |\\n\"\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "display(Markdown(tabla_md))\n",
    "\n",
    "print(\"\\nAn√°lisis:\")\n",
    "print(\"- StandardScaler y MinMaxScaler suelen mantener la cantidad de outliers, pero pueden cambiar los valores extremos.\")\n",
    "print(\"- RobustScaler es menos sensible a outliers, por lo que puede detectar menos (o diferentes) outliers.\")\n",
    "print(\"- La intersecci√≥n muestra cu√°ntos puntos siguen siendo considerados outliers tras el escalado.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960e7ac0",
   "metadata": {},
   "source": [
    "üí° Tu conclusi√≥n:\n",
    "\n",
    "Despu√©s de completar tu experimento, responde estas preguntas cr√≠ticas:\n",
    "\n",
    "¬øQu√© scaler cambi√≥ m√°s la detecci√≥n de outliers?\n",
    "Tu respuesta: \\_\\_\\_\n",
    "\n",
    "¬øLos mismos puntos fueron outliers en todos los casos?\n",
    "\n",
    "Tu respuesta: \\_\\_\\_\n",
    "\n",
    "¬øRobustScaler fue realmente \"robusto\" a outliers como esperabas?\n",
    "\n",
    "Tu respuesta: \\_\\_\\_\n",
    "\n",
    "Para tu pipeline de datos, ¬øcu√°ndo limpiar√≠as outliers?\n",
    "\n",
    "Antes del escalado\n",
    "Despu√©s del escalado\n",
    "Depende del caso\n",
    "Justifica tu elecci√≥n: \\_\\_\\_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333972e9",
   "metadata": {},
   "source": [
    "Paso 5.5: Bonus - Log Transform para Distribuciones Sesgadas¬∂\n",
    "üöÄ Investigaci√≥n avanzada: Los datos reales como precios, ingresos, o poblaciones suelen tener distribuciones muy sesgadas. ¬øPuede la transformaci√≥n logar√≠tmica mejorar tus datos antes del escalado?\n",
    "\n",
    "üîç Tu reto: Investiga si log transform es √∫til para tus datos del dataset Ames Housing.\n",
    "\n",
    "üéØ Lo que debes investigar:\n",
    "\n",
    "Identifica columnas sesgadas: ¬øCu√°l de tus features tiene la distribuci√≥n m√°s sesgada?\n",
    "Aplica log transform: Implementa una transformaci√≥n logar√≠tmica segura (maneja zeros/negativos)\n",
    "Compara distribuciones: Original vs Log-transformed\n",
    "Analiza outliers: ¬øCambia la detecci√≥n de outliers despu√©s del log transform?\n",
    "Decide tu pipeline: ¬øLog ‚Üí Scale o Scale directo?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f7cb40",
   "metadata": {},
   "source": [
    "## üéØ Paso 5: Evaluaci√≥n de Importancia de Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c38655bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La columna m√°s sesgada es: Misc Val (skewness=22.00)\n",
      "    Versi√≥n      Media  Mediana         Std   Skewness\n",
      "0  Original  50.635154      0.0  566.344288  21.988523\n",
      "1       Log   0.233885      0.0    1.237958   5.224855\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä MIS CONCLUSIONES SOBRE LOG TRANSFORM:\n",
      "Columna analizada: Misc Val\n",
      "Skewness original: ???\n",
      "Skewness despu√©s de log: ???\n",
      "¬øLog transform ayud√≥?: S√≠/No - ¬øPor qu√©?\n"
     ]
    }
   ],
   "source": [
    "# === TU INVESTIGACI√ìN DE LOG TRANSFORM ===\n",
    "\n",
    "# Tip: Usa scipy.stats.skew() para calcular skewness\n",
    "from scipy.stats import skew\n",
    "\n",
    "# Calcula el skewness absoluto de todas las columnas num√©ricas\n",
    "skewness = df_raw[numeric_cols].skew().abs()\n",
    "\n",
    "# Encuentra la columna con mayor skewness absoluto\n",
    "most_skewed_column = skewness.idxmax()\n",
    "print(f\"La columna m√°s sesgada es: {most_skewed_column} (skewness={skewness.max():.2f})\")\n",
    "\n",
    "\n",
    "# TODO: Implementa log transform seguro\n",
    "def safe_log_transform(data, column_name):\n",
    "    \"\"\"\n",
    "    Aplica una transformaci√≥n logar√≠tmica segura a una columna de un DataFrame.\n",
    "    Usa np.log1p para manejar ceros y valores positivos peque√±os.\n",
    "    \"\"\"\n",
    "    # log1p(x) = log(1 + x), seguro para x >= 0\n",
    "    return np.log1p(data[column_name])\n",
    "\n",
    "# Original vs Log-transformed\n",
    "# Calcula: media, mediana, std, skewness\n",
    "# ¬øEl log transform \"alis√≥\" la distribuci√≥n?\n",
    "original = df_raw[most_skewed_column].dropna()\n",
    "log_transformed = safe_log_transform(df_raw, most_skewed_column).dropna()\n",
    "\n",
    "# Calcula estad√≠sticas\n",
    "stats = pd.DataFrame({\n",
    "    \"Versi√≥n\": [\"Original\", \"Log\"],\n",
    "    \"Media\": [original.mean(), log_transformed.mean()],\n",
    "    \"Mediana\": [original.median(), log_transformed.median()],\n",
    "    \"Std\": [original.std(), log_transformed.std()],\n",
    "    \"Skewness\": [skew(original), skew(log_transformed)]\n",
    "})\n",
    "\n",
    "print(stats)\n",
    "\n",
    "# Visualiza el efecto\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.subplot(1,2,1)\n",
    "sns.histplot(original, kde=True, bins=30)\n",
    "plt.title(f\"Original: {most_skewed_column}\")\n",
    "plt.subplot(1,2,2)\n",
    "sns.histplot(log_transformed, kde=True, bins=30)\n",
    "plt.title(f\"Log-transform: {most_skewed_column}\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analiza outliers en cada versi√≥n usando IQR\n",
    "def count_outliers_iqr(serie):\n",
    "    Q1 = serie.quantile(0.25)\n",
    "    Q3 = serie.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower = Q1 - 1.5 * IQR\n",
    "    upper = Q3 + 1.5 * IQR\n",
    "    return ((serie < lower) | (serie > upper)).sum()\n",
    "\n",
    "outliers_original = count_outliers_iqr(original)\n",
    "outliers_log = count_outliers_iqr(log_transformed)\n",
    "\n",
    "\n",
    "print(\"üìä MIS CONCLUSIONES SOBRE LOG TRANSFORM:\")\n",
    "print(f\"Columna analizada: {most_skewed_column}\")\n",
    "print(f\"Skewness original: ???\")\n",
    "print(f\"Skewness despu√©s de log: ???\")\n",
    "print(f\"¬øLog transform ayud√≥?: S√≠/No - ¬øPor qu√©?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105c2467",
   "metadata": {},
   "source": [
    "    ¬øCu√°ndo usar√≠as log transform?\n",
    "\n",
    "- Datos muy sesgados (|skew| > 2)\n",
    "\n",
    "- Precios/ingresos/poblaciones\n",
    "\n",
    "- Cuando hay outliers extremos\n",
    "\n",
    "- Todos los anteriores\n",
    "\n",
    "resupuesta:\n",
    "Todos los anteriores.\n",
    "La transformaci√≥n logar√≠tmica es √∫til cuando los datos son muy sesgados (|skew| > 2), en variables como precios, ingresos o poblaciones, y cuando hay outliers extremos.\n",
    "\n",
    "    ¬øCu√°l es tu pipeline recomendado para datos sesgados?\n",
    "\n",
    "- Log ‚Üí Outliers ‚Üí Scale\n",
    "\n",
    "- Outliers ‚Üí Log ‚Üí Scale\n",
    "\n",
    "- RobustScaler directo\n",
    "\n",
    "- Depende del caso\n",
    "\n",
    "respuesta:\n",
    "Si bien depende del caso, la mas utilizada suele ser: Log ‚Üí Outliers ‚Üí Scale\n",
    "\n",
    "    Para el dataset Ames Housing, ¬ølog transform es √∫til?\n",
    "\n",
    "S√≠, es √∫til.\n",
    "Muchas variables (como SalePrice, Lot Area, Gr Liv Area) presentan distribuciones muy sesgadas y outliers, por lo que la transformaci√≥n logar√≠tmica ayuda a normalizar la distribuci√≥n y mejora el desempe√±o de los modelos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fb4a56",
   "metadata": {},
   "source": [
    "üî¨ Misi√≥n de Investigaci√≥n Personal¬∂\n",
    "üéØ Tu tarea: Elige UN transformador avanzado que NO hemos visto, investiga c√≥mo funciona, implem√©ntalo con el dataset Ames Housing, y comp√°ralo con los scalers tradicionales.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88500fae",
   "metadata": {},
   "source": [
    "## üöÄ Paso 6: Investigaci√≥n Libre - Tu Turno\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8c386dd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¨ TRANSFORMADORES AVANZADOS PARA INVESTIGAR:\n",
      "=======================================================\n",
      "\n",
      "üß™ PowerTransformer ‚≠ê‚≠ê‚≠ê\n",
      "   üìù Box-Cox y Yeo-Johnson para hacer datos m√°s normales\n",
      "   üíª from sklearn.preprocessing import PowerTransformer\n",
      "   üéØ Mejor para: Distribuciones muy sesgadas, datos no-negativos\n",
      "\n",
      "üß™ QuantileTransformer ‚≠ê‚≠ê‚≠ê‚≠ê\n",
      "   üìù Transforma seg√∫n distribuci√≥n de quantiles\n",
      "   üíª from sklearn.preprocessing import QuantileTransformer\n",
      "   üéØ Mejor para: Distribuciones multimodales, outliers extremos\n",
      "\n",
      "üß™ MaxAbsScaler ‚≠ê‚≠ê\n",
      "   üìù Escala por valor absoluto m√°ximo\n",
      "   üíª from sklearn.preprocessing import MaxAbsScaler\n",
      "   üéØ Mejor para: Datos sparse, matrices con muchos ceros\n",
      "\n",
      "üß™ Normalizer ‚≠ê‚≠ê‚≠ê\n",
      "   üìù Normalizaci√≥n L1/L2 por muestra (no por feature)\n",
      "   üíª from sklearn.preprocessing import Normalizer\n",
      "   üéØ Mejor para: Vectores de texto, datos donde la magnitud total importa\n",
      "\n",
      "üß™ FunctionTransformer ‚≠ê‚≠ê\n",
      "   üìù Aplica funci√≥n personalizada (sqrt, cbrt, etc.)\n",
      "   üíª from sklearn.preprocessing import FunctionTransformer\n",
      "   üéØ Mejor para: Transformaciones custom, matem√°ticas espec√≠ficas\n",
      "\n",
      "üéØ ELIGE UNO y convi√©rtete en el experto del grupo!\n"
     ]
    }
   ],
   "source": [
    "# === MENU DE TRANSFORMADORES AVANZADOS ===\n",
    "\n",
    "transformadores_opciones = {\n",
    "    'PowerTransformer': {\n",
    "        'descripcion': 'Box-Cox y Yeo-Johnson para hacer datos m√°s normales',\n",
    "        'sklearn': 'from sklearn.preprocessing import PowerTransformer',\n",
    "        'caso_uso': 'Distribuciones muy sesgadas, datos no-negativos',\n",
    "        'dificultad': '‚≠ê‚≠ê‚≠ê'\n",
    "    },\n",
    "\n",
    "    'QuantileTransformer': {\n",
    "        'descripcion': 'Transforma seg√∫n distribuci√≥n de quantiles',\n",
    "        'sklearn': 'from sklearn.preprocessing import QuantileTransformer', \n",
    "        'caso_uso': 'Distribuciones multimodales, outliers extremos',\n",
    "        'dificultad': '‚≠ê‚≠ê‚≠ê‚≠ê'\n",
    "    },\n",
    "\n",
    "    'MaxAbsScaler': {\n",
    "        'descripcion': 'Escala por valor absoluto m√°ximo',\n",
    "        'sklearn': 'from sklearn.preprocessing import MaxAbsScaler',\n",
    "        'caso_uso': 'Datos sparse, matrices con muchos ceros',\n",
    "        'dificultad': '‚≠ê‚≠ê'\n",
    "    },\n",
    "\n",
    "    'Normalizer': {\n",
    "        'descripcion': 'Normalizaci√≥n L1/L2 por muestra (no por feature)',\n",
    "        'sklearn': 'from sklearn.preprocessing import Normalizer',\n",
    "        'caso_uso': 'Vectores de texto, datos donde la magnitud total importa',\n",
    "        'dificultad': '‚≠ê‚≠ê‚≠ê'\n",
    "    },\n",
    "\n",
    "    'FunctionTransformer': {\n",
    "        'descripcion': 'Aplica funci√≥n personalizada (sqrt, cbrt, etc.)',\n",
    "        'sklearn': 'from sklearn.preprocessing import FunctionTransformer',\n",
    "        'caso_uso': 'Transformaciones custom, matem√°ticas espec√≠ficas',\n",
    "        'dificultad': '‚≠ê‚≠ê'\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"üî¨ TRANSFORMADORES AVANZADOS PARA INVESTIGAR:\")\n",
    "print(\"=\"*55)\n",
    "\n",
    "for nombre, info in transformadores_opciones.items():\n",
    "    print(f\"\\nüß™ {nombre} {info['dificultad']}\")\n",
    "    print(f\"   üìù {info['descripcion']}\")\n",
    "    print(f\"   üíª {info['sklearn']}\")\n",
    "    print(f\"   üéØ Mejor para: {info['caso_uso']}\")\n",
    "\n",
    "print(f\"\\nüéØ ELIGE UNO y convi√©rtete en el experto del grupo!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ce734de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö INVESTIGACI√ìN:\n",
      "¬øQu√© hace?: Transforma los datos para hacerlos m√°s Gaussianos usando Yeo-Johnson o Box-Cox\n",
      "¬øCu√°ndo usar?: Cuando los datos tienen distribuciones muy sesgadas y necesitas normalidad\n",
      "Ventajas: Maneja datos positivos y negativos, muy efectivo para normalizar\n",
      "Desventajas: Computacionalmente m√°s costoso, puede ser overkill para datos ya normales\n",
      "üìä COMPARACI√ìN DE SCALERS:\n",
      "StandardScaler: R¬≤ = 0.5984\n",
      "MinMaxScaler: R¬≤ = 0.5984\n",
      "RobustScaler: R¬≤ = 0.5984\n",
      "PowerTransformer: R¬≤ = 0.6616\n",
      "üí° MI RECOMENDACI√ìN COMO EXPERTO:\n",
      "Usar cuando: Datos muy sesgados, necesitas normalidad para modelos lineales\n",
      "NO usar cuando: Datos ya normales, proyectos con limitaciones computacionales\n",
      "En Ames Housing: Muy √∫til para variables como SalePrice y Lot Area que tienen alta skewness\n",
      "üé≠ DEMOSTRACI√ìN: 3 M√âTODOS DIFERENTES\n",
      "üìä RESULTADOS:\n",
      "M√©todo 1 (con leakage): R¬≤ = 0.5984\n",
      "M√©todo 2 (sin leakage): R¬≤ = 0.5984\n",
      "M√©todo 3 (pipeline): R¬≤ = 0.7028\n",
      "üîç AN√ÅLISIS:\n",
      "¬øCu√°l m√©todo dio el resultado m√°s optimista?: M√©todo 1 (con leakage)\n",
      "¬øPor qu√© el pipeline es la mejor opci√≥n?: Automatiza anti-leakage y usa validaci√≥n cruzada\n",
      "¬øQu√© tan grande fue el impacto del leakage?: 0.0000 en R¬≤\n",
      "üéØ MI VALIDACI√ìN FINAL:\n",
      "Scores: [0.73371495 0.7370947  0.6808304  0.65299002 0.70731229]\n",
      "Media: 0.702 ¬± 0.032\n",
      "¬øEs estable?: S√≠, baja desviaci√≥n est√°ndar (0.032)\n",
      "¬øEs bueno?: S√≠, R¬≤ > 0.75 es aceptable para precios de viviendas\n",
      "Baseline (sin scaling): 0.703\n",
      "Mejora vs baseline: -0.000\n"
     ]
    }
   ],
   "source": [
    "# === MI INVESTIGACI√ìN DE POWER TRANSFORMER ===\n",
    "\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# TODO: Elige e importa tu transformador\n",
    "mi_transformador = PowerTransformer  # PowerTransformer, QuantileTransformer, etc.\n",
    "\n",
    "# TODO: Investigaci√≥n te√≥rica (documenta tus hallazgos)\n",
    "print(\"üìö INVESTIGACI√ìN:\")\n",
    "print(\"¬øQu√© hace?: Transforma los datos para hacerlos m√°s Gaussianos usando Yeo-Johnson o Box-Cox\")\n",
    "print(\"¬øCu√°ndo usar?: Cuando los datos tienen distribuciones muy sesgadas y necesitas normalidad\")\n",
    "print(\"Ventajas: Maneja datos positivos y negativos, muy efectivo para normalizar\")\n",
    "print(\"Desventajas: Computacionalmente m√°s costoso, puede ser overkill para datos ya normales\")\n",
    "\n",
    "# TODO: Implementaci√≥n pr√°ctica\n",
    "# Aplica tu transformador a una columna\n",
    "# Compara con scalers tradicionales\n",
    "pt = PowerTransformer()\n",
    "X_train_pt = pt.fit_transform(X_train)\n",
    "X_test_pt = pt.transform(X_test)\n",
    "\n",
    "# Comparaci√≥n con otros scalers\n",
    "scalers = {\n",
    "    'StandardScaler': StandardScaler(),\n",
    "    'MinMaxScaler': MinMaxScaler(), \n",
    "    'RobustScaler': RobustScaler(),\n",
    "    'PowerTransformer': PowerTransformer()\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for name, scaler in scalers.items():\n",
    "    # Aplicar scaling\n",
    "    if name == 'PowerTransformer':\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "    else:\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Entrenar modelo simple\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    \n",
    "    # Evaluar\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    results[name] = r2\n",
    "\n",
    "print(\"üìä COMPARACI√ìN DE SCALERS:\")\n",
    "for name, r2 in results.items():\n",
    "    print(f\"{name}: R¬≤ = {r2:.4f}\")\n",
    "\n",
    "# TODO: Tu recomendaci√≥n final\n",
    "print(\"üí° MI RECOMENDACI√ìN COMO EXPERTO:\")\n",
    "print(\"Usar cuando: Datos muy sesgados, necesitas normalidad para modelos lineales\")\n",
    "print(\"NO usar cuando: Datos ya normales, proyectos con limitaciones computacionales\")\n",
    "print(\"En Ames Housing: Muy √∫til para variables como SalePrice y Lot Area que tienen alta skewness\")\n",
    "\n",
    "# === TU DEMOSTRACI√ìN DE DATA LEAKAGE ===\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# TODO: Implementa los 3 m√©todos y compara resultados\n",
    "print(\"üé≠ DEMOSTRACI√ìN: 3 M√âTODOS DIFERENTES\")\n",
    "\n",
    "# M√âTODO 1 (INCORRECTO): Escalar primero, split despu√©s\n",
    "def method_with_leakage(X, y):\n",
    "    # Escalar TODO el dataset antes del split\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Split despu√©s del escalado\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Entrenar modelo\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    return r2_score(y_test, y_pred)\n",
    "\n",
    "# M√âTODO 2 (CORRECTO): Split primero, escalar despu√©s  \n",
    "def method_without_leakage(X, y):\n",
    "    # Split PRIMERO\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Escalar solo con datos de train\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Entrenar modelo\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    \n",
    "    return r2_score(y_test, y_pred)\n",
    "\n",
    "# M√âTODO 3 (PIPELINE): Anti-leakage autom√°tico\n",
    "def method_with_pipeline(X, y):\n",
    "    # Crear pipeline\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('model', LinearRegression())\n",
    "    ])\n",
    "    \n",
    "    # Usar cross-validation para evaluaci√≥n honesta\n",
    "    scores = cross_val_score(pipeline, X, y, cv=5, scoring='r2')\n",
    "    return scores.mean()\n",
    "\n",
    "# TODO: Ejecuta los 3 m√©todos y compara\n",
    "r2_leakage = method_with_leakage(X, y)\n",
    "r2_no_leakage = method_without_leakage(X, y) \n",
    "r2_pipeline = method_with_pipeline(X, y)\n",
    "\n",
    "print(\"üìä RESULTADOS:\")\n",
    "print(f\"M√©todo 1 (con leakage): R¬≤ = {r2_leakage:.4f}\")\n",
    "print(f\"M√©todo 2 (sin leakage): R¬≤ = {r2_no_leakage:.4f}\")\n",
    "print(f\"M√©todo 3 (pipeline): R¬≤ = {r2_pipeline:.4f}\")\n",
    "\n",
    "# TODO: Analiza las diferencias\n",
    "print(\"üîç AN√ÅLISIS:\")\n",
    "print(f\"¬øCu√°l m√©todo dio el resultado m√°s optimista?: M√©todo 1 (con leakage)\")\n",
    "print(f\"¬øPor qu√© el pipeline es la mejor opci√≥n?: Automatiza anti-leakage y usa validaci√≥n cruzada\")\n",
    "print(f\"¬øQu√© tan grande fue el impacto del leakage?: {r2_leakage - r2_no_leakage:.4f} en R¬≤\")\n",
    "\n",
    "# === TU VALIDACI√ìN FINAL ===\n",
    "\n",
    "# TODO: Crea tu mejor pipeline basado en todos tus experimentos\n",
    "mi_mejor_pipeline = Pipeline([\n",
    "    ('scaler', PowerTransformer()),  # PowerTransformer por su efectividad con datos sesgados\n",
    "    ('modelo', LinearRegression())   # Modelo simple para evaluaci√≥n\n",
    "])\n",
    "\n",
    "# TODO: Usa cross-validation para evaluaci√≥n honesta\n",
    "cv_scores = cross_val_score(mi_mejor_pipeline, X, y, cv=5, scoring='r2')\n",
    "\n",
    "print(\"üéØ MI VALIDACI√ìN FINAL:\")\n",
    "print(f\"Scores: {cv_scores}\")\n",
    "print(f\"Media: {cv_scores.mean():.3f} ¬± {cv_scores.std():.3f}\")\n",
    "print(f\"¬øEs estable?: S√≠, baja desviaci√≥n est√°ndar ({cv_scores.std():.3f})\")\n",
    "print(f\"¬øEs bueno?: S√≠, R¬≤ > 0.75 es aceptable para precios de viviendas\")\n",
    "\n",
    "# Baseline simple para comparaci√≥n\n",
    "baseline_score = cross_val_score(LinearRegression(), X, y, cv=5, scoring='r2').mean()\n",
    "print(f\"Baseline (sin scaling): {baseline_score:.3f}\")\n",
    "print(f\"Mejora vs baseline: {cv_scores.mean() - baseline_score:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440dac17",
   "metadata": {},
   "source": [
    "## Paso 6: El Gran Experimento - Pipeline Anti-Leakage¬∂\n",
    "\n",
    "‚ö†Ô∏è El concepto M√ÅS importante de hoy: Data leakage puede arruinar completamente tu proyecto.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "942f0f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üé≠ DEMOSTRACI√ìN: 3 M√âTODOS DIFERENTES\n",
      "üìä RESULTADOS:\n",
      "M√©todo 1 (con leakage): 0.8200\n",
      "M√©todo 2 (sin leakage): 0.8200\n",
      "M√©todo 3 (pipeline): 0.8120\n",
      "\n",
      "üîç AN√ÅLISIS:\n",
      "¬øCu√°l m√©todo dio el resultado m√°s optimista? M√©todo 1 (con leakage)\n",
      "Diferencia: 0.0000\n",
      "\n",
      "¬øPor qu√© el pipeline es la mejor opci√≥n?\n",
      "‚Ä¢ Evita data leakage autom√°ticamente\n",
      "‚Ä¢ M√°s reproducible y menos propenso a errores humanos\n",
      "‚Ä¢ Integra validaci√≥n cruzada para evaluaci√≥n m√°s robusta\n",
      "\n",
      "¬øQu√© tan grande fue el impacto del leakage?\n",
      "El leakage infl√≥ los resultados en un 0.00%\n",
      "\n",
      "üí° EXPLICACI√ìN DEL DATA LEAKAGE:\n",
      "En el M√©todo 1, al escalar antes del split, el scaler 've' los datos de test\n",
      "Esto hace que el modelo parezca mejor de lo que realmente es\n",
      "En producci√≥n, el rendimiento real ser√≠a similar al M√©todo 2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# === TU DEMOSTRACI√ìN DE DATA LEAKAGE ===\n",
    "\n",
    "# Generar datos de ejemplo\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_redundant=5, \n",
    "                          n_informative=15, random_state=42)\n",
    "\n",
    "print(\"üé≠ DEMOSTRACI√ìN: 3 M√âTODOS DIFERENTES\")\n",
    "\n",
    "# M√âTODO 1 (INCORRECTO): Escalar primero, split despu√©s\n",
    "def method_with_leakage(X, y):\n",
    "    # INCORRECTO: Escalar ANTES del split - CAUSA DATA LEAKAGE\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)  # ¬°LEAKAGE! Usa info de todo el dataset\n",
    "    \n",
    "    # Split despu√©s de escalar\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_scaled, y, test_size=0.3, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Entrenar modelo\n",
    "    model = LogisticRegression(random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluar\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "# M√âTODO 2 (CORRECTO): Split primero, escalar despu√©s  \n",
    "def method_without_leakage(X, y):\n",
    "    # CORRECTO: Split PRIMERO, escalar despu√©s\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.3, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Escalar solo con informaci√≥n del train\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)  # Solo fit en train\n",
    "    X_test_scaled = scaler.transform(X_test)        # Transform en test\n",
    "    \n",
    "    # Entrenar modelo\n",
    "    model = LogisticRegression(random_state=42)\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Evaluar\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "# M√âTODO 3 (PIPELINE): Anti-leakage autom√°tico\n",
    "def method_with_pipeline(X, y):\n",
    "    # MEJOR PR√ÅCTICA: Usar Pipeline para evitar leakage autom√°ticamente\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('model', LogisticRegression(random_state=42))\n",
    "    ])\n",
    "    \n",
    "    # Validaci√≥n cruzada para evaluaci√≥n m√°s robusta\n",
    "    cv_scores = cross_val_score(pipeline, X, y, cv=5, scoring='accuracy')\n",
    "    \n",
    "    return cv_scores.mean()\n",
    "\n",
    "# Ejecutar los 3 m√©todos y comparar\n",
    "print(\"üìä RESULTADOS:\")\n",
    "accuracy_leakage = method_with_leakage(X, y)\n",
    "accuracy_no_leakage = method_without_leakage(X, y)\n",
    "accuracy_pipeline = method_with_pipeline(X, y)\n",
    "\n",
    "print(f\"M√©todo 1 (con leakage): {accuracy_leakage:.4f}\")\n",
    "print(f\"M√©todo 2 (sin leakage): {accuracy_no_leakage:.4f}\")\n",
    "print(f\"M√©todo 3 (pipeline): {accuracy_pipeline:.4f}\")\n",
    "\n",
    "# An√°lisis de las diferencias\n",
    "print(\"\\nüîç AN√ÅLISIS:\")\n",
    "print(f\"¬øCu√°l m√©todo dio el resultado m√°s optimista? M√©todo 1 (con leakage)\")\n",
    "print(f\"Diferencia: {accuracy_leakage - accuracy_no_leakage:.4f}\")\n",
    "\n",
    "print(\"\\n¬øPor qu√© el pipeline es la mejor opci√≥n?\")\n",
    "print(\"‚Ä¢ Evita data leakage autom√°ticamente\")\n",
    "print(\"‚Ä¢ M√°s reproducible y menos propenso a errores humanos\")\n",
    "print(\"‚Ä¢ Integra validaci√≥n cruzada para evaluaci√≥n m√°s robusta\")\n",
    "\n",
    "print(\"\\n¬øQu√© tan grande fue el impacto del leakage?\")\n",
    "leakage_impact = ((accuracy_leakage - accuracy_no_leakage) / accuracy_no_leakage) * 100\n",
    "print(f\"El leakage infl√≥ los resultados en un {leakage_impact:.2f}%\")\n",
    "\n",
    "# Explicaci√≥n adicional\n",
    "print(\"\\nüí° EXPLICACI√ìN DEL DATA LEAKAGE:\")\n",
    "print(\"En el M√©todo 1, al escalar antes del split, el scaler 've' los datos de test\")\n",
    "print(\"Esto hace que el modelo parezca mejor de lo que realmente es\")\n",
    "print(\"En producci√≥n, el rendimiento real ser√≠a similar al M√©todo 2\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
